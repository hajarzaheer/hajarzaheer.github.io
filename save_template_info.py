import os
import sys
import pickle
import gzip
import argparse
import pandas as pd
import glob
import subprocess
import requests
import re
import torch
import numpy as np
import traceback
sys.path.append("/work/users/h/a/hajar/rfaa_tf/rf2aa/")
sys.path.append("/work/users/h/a/hajar/rfaa_tf/")
from chemical import num2aa, aa2num, NTOTAL, aa2long
sys.path.append("/work/users/a/m/amritan/pdbx/")
from pdbx.reader.PdbxReader import PdbxReader

# Reads the contents of a cif file and returns: xyz coords, atom mask, residue indices, and sequence.
# Adapted from Baker lab: https://github.com/baker-laboratory/RoseTTAFold-All-Atom/blob/main/rf2aa/data/parsers.py
def parse_pdbx_lines_w_seq(cif, authch, lddtmask=False):
    data = []
    reader = PdbxReader(cif)
    reader.read(data)

    data = data[0]

    atom_site = data.getObj('atom_site')

    res = []
    for r in atom_site.getRowList():
        label_atom_id = r[atom_site.getIndex('label_atom_id')]
        label_asym_id = r[atom_site.getIndex('label_asym_id')]
        auth_asym_id = r[atom_site.getIndex('auth_asym_id')]
        if r[atom_site.getIndex('group_PDB')] == 'ATOM' and \
            label_atom_id in ['CA', 'P'] and \
                auth_asym_id == authch:
            res.append((
                label_asym_id,
                r[atom_site.getIndex('label_seq_id')],
                r[atom_site.getIndex('label_comp_id')],
                r[atom_site.getIndex('B_iso_or_equiv')]
            ))
    # indices of residues observed in the structure
    # (chain letter, res num, aa, bfactor / plddt)
    pdb_idx_s = [(r[0], int(r[1])) for r in res]

    idx_s = [int(r[1]) for r in res]
    plddt = [float(r[3]) for r in res]
    seq = [aa2num[r[2]] if r[2] in aa2num.keys() else 20 for r in res]

    # 4 BB + up to 10 SC atoms
    xyz = np.full((len(idx_s), NTOTAL, 3), np.nan, dtype=np.float32)
    for r in atom_site.getRowList():
        auth_asym_id = r[atom_site.getIndex('auth_asym_id')]

        if r[atom_site.getIndex('group_PDB')] != 'ATOM' or auth_asym_id != authch:
            continue
        chain, resNo, atom, aa = \
            r[atom_site.getIndex('label_asym_id')], \
            int(r[atom_site.getIndex('label_seq_id')]), \
            r[atom_site.getIndex('label_atom_id')], \
            r[atom_site.getIndex('label_comp_id')]
        try:
            idx = pdb_idx_s.index((chain, resNo))
        except Exception as e:
            print(repr(e))
            #print(traceback.format_exc())
        for i_atm, tgtatm in enumerate(aa2long[aa2num[aa]]):
            if tgtatm == atom:
                x = r[atom_site.getIndex('Cartn_x')]
                y = r[atom_site.getIndex('Cartn_y')]
                z = r[atom_site.getIndex('Cartn_z')]
                xyz[idx, i_atm, :] = [float(x), float(y), float(z)]
                break

    # save atom mask
    mask = np.logical_not(np.isnan(xyz[..., 0]))
    xyz[np.isnan(xyz[..., 0])] = 0.0

    if lddtmask:
        # update mask info with plddt (ignore sidechains if plddt < 85.0 and all if plddt < 70.0)
        mask_lddt = np.full_like(mask, False)
        mask_lddt[plddt > 85.0] = True
        mask_lddt[plddt > 70.0, 5:] = True
        mask = np.logical_and(mask, mask_lddt)

    return xyz, mask, np.array(idx_s), np.array(seq)

# Reads .hhr and .atab files generated by hhsearch, processes .cif files for top 20 (or max_templ) hits and returns templates features to be stored. 
# Adapted from Baker Lab: https://github.com/baker-laboratory/RoseTTAFold-All-Atom/blob/main/rf2aa/data/parsers.py
def parse_templates_raw(hhr_fn, atab_fn, max_templ=20):
    # process tabulated hhsearch output to get
    # matched positions and positional scores
    hits = []
    for l in open(atab_fn, "r").readlines():
        if l[0] == '>':
            if len(hits) == max_templ:
                break
            key = l[1:].split()[0]
            hits.append([key, [], []])
        elif "score" in l or "dssp" in l:
            continue
        else:
            hi = l.split()[:5]+[0.0, 0.0, 0.0]
            hits[-1][1].append([int(hi[0]), int(hi[1])])
            hits[-1][2].append([float(hi[2]), float(hi[3]), float(hi[4])])

    # get per-hit statistics from an .hhr file
    # (!!! assume that .hhr and .atab have the same hits !!!)
    # [Probab, E-value, Score, Aligned_cols,
    # Identities, Similarity, Sum_probs, Template_Neff]
    lines = open(hhr_fn, "r").readlines()
    pos = [i+1 for i, l in enumerate(lines) if l[0] == '>']
    for i, posi in enumerate(pos[:len(hits)]):
        hits[i].append([float(s)
                       for s in re.sub('[=%]', ' ', lines[posi]).split()[1::2]])

    for hi in hits:
        pdbid, authch = hi[0].split('_')

        #print("Read %s..." % hi[0])

        resp = requests.get(f'https://files.rcsb.org/view/{pdbid}.cif')
        if resp.ok:
            lines = resp.text
            cif = lines.split('\n')
            hi += list(parse_pdbx_lines_w_seq(cif, authch))

    # process hits
    counter = 0
    xyz, qmap, mask, f0d, f1d, ids, seq = [], [], [], [], [], [], []
    for data in hits:
        if len(data) < 7:
            continue
        print("Process %s..." % data[0])

        qi, ti = np.array(data[1]).T
        _, sel1, sel2 = np.intersect1d(ti, data[6], return_indices=True)
        ncol = sel1.shape[0]
        if ncol < 10:
            continue

        ids.append(data[0])
        f0d.append(data[3])
        f1d.append(np.array(data[2])[sel1])
        xyz.append(data[4][sel2])
        mask.append(data[5][sel2])
        seq.append(data[-1][sel2])
        qmap.append(np.stack([qi[sel1]-1, [counter]*ncol], axis=-1))
        counter += 1

    xyz = np.vstack(xyz).astype(np.float32)
    mask = np.vstack(mask).astype(bool)
    qmap = np.vstack(qmap).astype(np.int32)
    f0d = np.vstack(f0d).astype(np.float32)
    f1d = np.vstack(f1d).astype(np.float32)
    seq = np.hstack(seq).astype(np.int32)
    ids = ids

    return torch.from_numpy(xyz), torch.from_numpy(mask), torch.from_numpy(qmap), \
        torch.from_numpy(f0d), torch.from_numpy(
            f1d), torch.from_numpy(seq), ids


parser = argparse.ArgumentParser()
parser.add_argument("-istart", type=int, default=0)
parser.add_argument("-num", type=int, default=1)
args = parser.parse_args()

templs = pd.read_csv(
    '/work/users/h/a/hajar/rcsb_proteins_5-25-22/templates_exist.csv', index_col=None)

for i, r in templs.iloc[args.istart:args.istart+args.num].iterrows():
    auth = r['AUTH']
    hash = r['HASH']

    pdbid, ch = auth.split('_')
    pth = f'/work/users/h/a/hajar/rcsb_proteins_5-25-22/pdb/{pdbid.lower()}_{ch}'

    hhr_fn = f'{pth}/hhr/pdb70_hits2.hhr'
    atab_fn = f'{pth}/hhr/pdb70_hits2.atab'

    atab_exist = os.path.isfile(atab_fn)

    if atab_exist:
        print('ID:', pdbid)
        try:
            xyz, mask, qmap, f0d, f1d, seq, ids = parse_templates_raw(
                hhr_fn, atab_fn, max_templ=20)

            outfn = f'/work/users/h/a/hajar/rcsb_proteins_5-25-22/TPLT/{str(hash)[:3]}/{hash}.pt'

            print(outfn)
            os.makedirs(os.path.dirname(outfn), exist_ok=True)

            torch.save(
                {
                    'xyz': xyz,
                    'mask': mask,
                    'qmap': qmap,
                    'f0d': f0d,
                    'f1d': f1d,
                    'seq': seq,
                    'ids': ids,
                },
                outfn
            )

        except Exception as e:
            print(repr(e))

            print(traceback.format_exc())
